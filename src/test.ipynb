{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aacae6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (rotary_emb): Qwen3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "query = \"Give me a short introduction to large language model.\"\n",
    "\n",
    "# model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "# model_name = \"Qwen/Qwen3-4B\" \n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig( # quantitazation (q in qlora) part\n",
    "  load_in_4bit=True,\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "  bnb_4bit_use_double_quant=True,\n",
    "  bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "qwen = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  quantization_config=bnb_config,\n",
    "  device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(qwen, \"../out/checkpoint-114\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ff77c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Give me a short introduction to large language model.',\n",
       " 'output': 'A large language model (LLM) is a machine learning system designed to understand and generate human-like text in natural conversations, including writing, reading, and other natural language tasks. These models are trained on massive datasets and are capable of understanding context, sarcasm, and cultural nuances. They are used for a wide range of tasks, from content creation to language translation and information retrieval.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(tokenizer, prompt, thinking = False):\n",
    "  messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    "  text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=thinking\n",
    "  )\n",
    "  return tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "def generate(model, tokenizer, prompt, thinking = False):\n",
    "  tokens = tokenize(tokenizer, prompt, thinking)\n",
    "  generated_ids = model.generate(\n",
    "    **tokens,\n",
    "    max_new_tokens=32768\n",
    "  )\n",
    "  output_ids = generated_ids[0][len(tokens.input_ids[0]):].tolist() \n",
    "\n",
    "  # parsing thinking content\n",
    "  try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "  except ValueError:\n",
    "    index = 0\n",
    "  \n",
    "  return {\n",
    "    \"thinking_content\" : tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\"),\n",
    "    \"content\" : tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "  }\n",
    "\n",
    "model.eval()\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "content = generate(model, tokenizer, prompt, False)\n",
    "{\n",
    "  \"prompt\" :prompt,\n",
    "  \"output\" : content[\"content\"]\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
